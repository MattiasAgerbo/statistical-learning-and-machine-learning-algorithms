{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Content:\n",
    "1. Detailed example: Two-layer MLP for regression\n",
    "    - Forward pass: Calculate the values of $z_1$, $z_2$, and $y$\n",
    "    - Compute the mean squared error\n",
    "    - Using backpropagation, compute the gradient or the error w.r.t the weights $w^{(2)}_2$ and $w^{(1)}_{2,2}$ \n",
    "    - Compute the updated weights for $w^{(2)}_2$ and $w^{(1)}_{2,2}$ \n",
    "2. PyTorch: 2-layer MLP for classification\n",
    "    - Create and train a 2-layer MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Detailed example: Two-layer MLP for regression\n",
    "We'll be working through a forward and back-propagation example in all its details for a 2-layer MLP for regression. Our network has the following structure:\n",
    "\n",
    "![](two-layer-nn.svg)\n",
    "\n",
    "Where \n",
    "    $$\n",
    "    z_j = \\text{ReLU}\\left(a_j \\right)\n",
    "    \\qquad\n",
    "    a_j = \\sum_i w^{(1)}_{ij} x_i\n",
    "    \\qquad\n",
    "    y_j = \\sum_i w^{(2)}_{ij} z_i\n",
    "    $$\n",
    "and the biases \n",
    "    $$\n",
    "    x_0 = z_0 = 1\n",
    "    $$\n",
    "\n",
    "Suppose, we have the weights\n",
    "    $$\n",
    "    \\mathbf{W}^{(1)} = \\left[ \n",
    "        \\begin{matrix}\n",
    "        0.1 & 0.2\\\\\n",
    "        -1.1 & 1.2\\\\\n",
    "        -2.1 & 2.2\n",
    "        \\end{matrix}\n",
    "    \\right]\n",
    "    \\qquad\n",
    "    \\mathbf{w}^{(2)} = \\left[ \n",
    "        \\begin{matrix}\n",
    "        -0.1\\\\\n",
    "        1.1 \\\\\n",
    "        2.2\n",
    "        \\end{matrix}\n",
    "    \\right]\n",
    "    $$\n",
    "Notice that the bias weights are included in the weight matrix. Relating it to the drawing, we have $w_{0,1} = 0.1$, $w_{1,2} = 1.2$\n",
    "\n",
    "Moreover, we are given an input\n",
    "    $$\n",
    "    \\mathbf{x} = \\left[ \n",
    "        \\begin{matrix}\n",
    "        0.1 \\\\\n",
    "        0.2\n",
    "        \\end{matrix}\n",
    "    \\right]\n",
    "    $$\n",
    "    \n",
    "Relating these to the drawing, we have $w^{(1)}_{1,2} = 1.2$ and $x_1 = 0.1$\n",
    "\n",
    "#### Matrix notation\n",
    "Notice we use matrix notation:\n",
    "$$\n",
    "W_k = \\begin{bmatrix}\n",
    "w^{(k)}_{0,1} & \\cdots & w^{(k)}_{0,l_k} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "w^{(k)}_{k_1-1,1} & \\cdots & w^{(k)}_{k-1,l_k}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "meaning the row indices start from 0 and the column indices start from 1.\n",
    "#### Forward Pass\n",
    "\n",
    "First, we define the activations for the first and second layers as a linear combination of inputs and weights, plus a bias term. For the first layer:\n",
    "\n",
    "$$\n",
    "a_1 = w_1^T \\begin{bmatrix} 1 \\\\ x \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This activation $ a_1 $ then passes through a non-linear activation function $ h $:\n",
    "\n",
    "$$\n",
    "z_1 = h(a_1)\n",
    "$$\n",
    "\n",
    "Similarly, for the second layer:\n",
    "\n",
    "$$\n",
    "a_2 = w_2^T \\begin{bmatrix} 1 \\\\ z_1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the output of the network $ y $ is obtained by passing $ a_2 $ through the activation function:\n",
    "\n",
    "$$\n",
    "y(x, \\{w_1, w_2\\}) = z_2 = h(a_2)\n",
    "$$\n",
    "\n",
    "#### Backward Pass\n",
    "\n",
    "During backpropagation, we first apply the chain rule to compute the gradient of the cost $ E $ w.r.t the weights $ W_2 $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W_2} = \\frac{\\partial a_2}{\\partial W_2} \\frac{\\partial E}{\\partial a_2}^T\n",
    "$$\n",
    "\n",
    "Similarly, we apply the chain rule for the weights $ W_1 $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W_1} = \\frac{\\partial a_1}{\\partial W_1} \\frac{\\partial E}{\\partial a_1}^T\n",
    "$$\n",
    "\n",
    "Now, let's detail the parts of these chain rule expansions:\n",
    "\n",
    "The gradient of the error w.r.t the output activation $ a_2 $ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial a_2} = y - t,\n",
    "$$\n",
    "\n",
    "and the gradient of $a_2$ w.r.t $w_2$ is:\n",
    "$$\n",
    "\\frac{\\partial a_2}{\\partial W_2} = \\begin{bmatrix} 1 \\\\ z_1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The gradient of the output activation $ a_2 $ w.r.t $ z_1 $ is represented by the weights $ W_2 $, since $ a_2 $ is a linear transformation of $ z_1 $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_1}{\\partial z_1} = W_2\n",
    "$$\n",
    "\n",
    "For the first layer, the derivative of $ z_1 $ w.r.t $ a_1 $ is the derivative of the Sigmoid activation function (see lecture slide for other actiavtion function derivatives):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_1}{\\partial a_1} = h(a_1) \\odot (1 - h(a_1)) = z_1 \\odot (1 - z_1)\n",
    "$$\n",
    "\n",
    "Finally, we find the gradient of the error $ E $ w.r.t the first layer activation $ a_1 $ by combining the previous terms:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial a_1} = \\frac{\\partial z_1}{\\partial a_1} \\frac{\\partial a_2}{\\partial z_1} \\frac{\\partial E}{\\partial a_2}\n",
    "$$\n",
    "\n",
    "These gradients are used to perform the weight updates in gradient descent to train our neural network.ate on both $ W_1 $ and $ W_2 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1) Forward pass: Calculate the values of $z_1$, $z_2$, and $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the values and formulas given above, as well as the ReLU activation function we determin the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1, z2, y: 0 0.7600000000000001 1.5720000000000003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0.1, 0.2])\n",
    "w1 = np.array([[0.1, 0.2], [-1.1, 1.2], [-2.1, 2.2]])\n",
    "w2 = np.array([-0.1, 1.1, 2.2])\n",
    "\n",
    "def ReLU(a1):\n",
    "    return [a if a > 0 else 0 for a in a1]\n",
    "\n",
    "a = w1.T@np.insert(x, 0, 1)\n",
    "\n",
    "z = ReLU(a)\n",
    "\n",
    "y = w2.T@np.insert(z, 0, 1)\n",
    "\n",
    "print(\"z1, z2, y:\", z[0],z[1],y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Compute the mean squared error\n",
    "Suppose our target $t=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error 0.09159199999999988\n"
     ]
    }
   ],
   "source": [
    "t = 2\n",
    "error = 1/2 * (y-t)**2\n",
    "print(\"The mean squared error\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Backpropagation and computeing the gradient or the error w.r.t the weights $w^{(2)}_2$ and $w^{(1)}_{2,2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of error w.r.t w^(2)_2: -0.32527999999999985\n",
      "Gradient of error w.r.t w^(1)_{2,2}: -0.1883199999999999\n"
     ]
    }
   ],
   "source": [
    "dE_dy = y - t\n",
    "\n",
    "dE_dw2_2 = dE_dy * z[1]\n",
    "\n",
    "dE_dw1_22 = dE_dy * w2[2] * (a[1] > 0) * x[1]\n",
    "\n",
    "print(\"Gradient of error w.r.t w^(2)_2:\", dE_dw2_2)\n",
    "print(\"Gradient of error w.r.t w^(1)_{2,2}:\", dE_dw1_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Compute the updated weights for $w^{(2)}_2$ and $w^{(1)}_{2,2}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated weights for w^{(2)}_2 and w^{(1)}_{2,2} 2.2325280000000003 2.181168\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1\n",
    "w2[2] -= eta * dE_dw2_2 \n",
    "w1[2,1] -= - eta * dE_dw1_22 \n",
    "\n",
    "print(\"The updated weights for w^{(2)}_2 and w^{(1)}_{2,2}\", w2[2], w1[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) PyTorch: 2-layer MLP for classification\n",
    "We'll be working with the classic MNIST dataset, which we can easily get via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 9912422/9912422 [00:00<00:00, 35673679.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 13926844.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 20543451.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = 64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few steps happened here:\n",
    "1. The dataset (train and test) was downloaded \n",
    "1. We created a `DataLoader` for each data split. Using this, we get batches of data (64 examples per batch here)\n",
    "1. We told asked for the training data to be shuffled\n",
    "\n",
    "Lets see what we get in a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "data.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Create and train a 2-layer MLP\n",
    "For the network, we use a single hidden layer of 512 neurons with a ReLU activation function for the first weight Linear layer. The output of the second Linear layer should be a softmax.\n",
    "\n",
    "For optimisation, we use the SGD optimizer with learning rate of 0.001, and the negative log-likelihood loss.\n",
    "\n",
    "The network is trained for 5 epochs on the train data, and the prediction accuracy on the test data is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Module\n",
    "from torch.nn.functional import relu, log_softmax, nll_loss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features=784, out_features=512)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(512, 10)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input data\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.log_softmax(x)\n",
    "        return output\n",
    "\n",
    "def train(\n",
    "    model:Module, \n",
    "    train_loader:DataLoader, \n",
    "    optimizer:SGD, \n",
    "    epoch:int, \n",
    "    log_interval = 50\n",
    "):\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feed the data through the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute the negative log-likelihood loss\n",
    "        loss = nll_loss(output, target)\n",
    "        \n",
    "        # Backward propagate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform an update step using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            # Log (Optional)\n",
    "            pass\n",
    "\n",
    "def test(model:Module, test_loader:DataLoader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Don't accumulate gradients\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Feed the data through the model\n",
    "            output = model(data)\n",
    "            \n",
    "            # Predict the class (it is the index of the max log-probability)\n",
    "            test_loss += nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            # Add to the number of correct\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # Print results\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, \n",
    "        len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8241/10000 (82%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 8666/10000 (87%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 8827/10000 (88%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 8926/10000 (89%)\n",
      "\n",
      "\n",
      "Test set: Accuracy: 8992/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, train_loader, optimizer, epoch, log_interval=50)\n",
    "    test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
